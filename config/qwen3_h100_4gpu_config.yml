# Qwen3:8B Security Fine-tuning Configuration for 4x H100 GPUs
# Optimized for penetration testing and security domain

base_model: Qwen/Qwen3-8B
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# Enhanced tokenizer with security domain tokens
tokenizer_config:
  path: ./tokenizer_security
  additional_special_tokens:
    - "IDOR"
    - "XSS" 
    - "SQLi"
    - "RCE"
    - "SSRF"
    - "LFI"
    - "RFI"
    - "XXE"
    - "CSRF"
    - "SSTI"
    - "NMAP"
    - "BURP"
    - "METASPLOIT"
    - "SQLMAP"
    - "WIRESHARK"
    - "NESSUS"
    - "GOBUSTER"
    - "FFUF"
    - "DIRB"
    - "CVE-"
    - "MSFVENOM"
    - "EXPLOITDB"
    - "SHODAN"
    - "ZAP"
    - "HYDRA"
    - "JOHN"
    - "HASHCAT"
    - "PRIVESC"
    - "PWN"
    - "ROP"
    - "SHELLCODE"
    - "BUFFER"
    - "OVERFLOW"
    - "PAYLOAD"
    - "REVSHELL"
    - "BINDSHELL"
    - "STAGED"
    - "STAGELESS"
    - "ADMIN:"
    - "ROOT:"
    - "FLAG{"
    - "HTTP/"
    - "HTTPS/"
    - "200OK"
    - "403FORBIDDEN"
    - "401UNAUTHORIZED"
    - "500ERROR"
    - "BASE64"
    - "MD5:"
    - "SHA1:"
    - "SHA256:"

# Dataset configuration
datasets:
  - path: ./data/augmented_training_data.jsonl
    type: chat_template
    chat_template: "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{{ '<|im_start|>assistant\n' }}"
    field: messages

dataset_prepared_path: ./last_run_prepared
val_set_size: 0.1  # Increased to 10% for better validation
output_dir: ./models/qwen3-finetuned

# H100-optimized training parameters
sequence_len: 8192
sample_packing: true
pad_to_sequence_len: true

# Optimized QLoRA parameters for 4x H100
adapter: lora
lora_r: 128  # Reduced for better generalization
lora_alpha: 256  # Reduced proportionally
lora_dropout: 0.1  # Increased for better regularization
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Optimized hyperparameters for 4,715 examples
learning_rate: 0.0001  # Reduced for stability
lr_scheduler: cosine_with_restarts  # Better for small datasets
num_epochs: 12  # Increased for better learning
micro_batch_size: 2  # Reduced for stability
gradient_accumulation_steps: 16  # Effective batch size = 2 * 16 * 4 = 128
eval_batch_size: 2
warmup_ratio: 0.1  # 10% of total steps for warmup
logging_steps: 10  # More frequent logging
save_steps: 50  # More frequent saving
eval_steps: 50  # More frequent evaluation

# H100-specific optimizations
bf16: true  # Use bfloat16 for H100
fp16: false
tf32: true  # Enable TF32 for H100
flash_attention: true  # Enable Flash Attention 2
gradient_checkpointing: true

# Special tokens for Qwen3
special_tokens:
  pad_token: ""
  bos_token: "<|im_start|>"
  eos_token: "<|im_end|>"
  unk_token: ""

# Advanced training settings
early_stopping_patience: 3  # Reduced for faster convergence
optimizer: adamw_torch
weight_decay: 0.05  # Increased for regularization
max_grad_norm: 0.5  # Reduced for stability
dataloader_pin_memory: true
dataloader_num_workers: 2  # Reduced for stability

# Distributed training configuration for 4x H100
device_map: auto
fsdp: 
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_cpu_ram_efficient_loading: true
  fsdp_sync_module_states: true
  fsdp_offload_params: false
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_use_orig_params: true

# Performance optimizations
rope_scaling:
  type: dynamic
  factor: 2.0

# Memory optimizations
max_memory_MB: 70000  # Reduced for stability
torch_compile: true  # Enable torch.compile for speed
torch_compile_backend: inductor

# Logging and monitoring
wandb_project: "qwen3-security-finetuning"
wandb_entity: null  # Set your wandb entity if needed
wandb_watch: gradients
logging_dir: ./logs

# Model saving
save_total_limit: 5  # Keep 5 best checkpoints
save_strategy: steps
evaluation_strategy: steps
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false

# Security domain specific settings
group_by_length: true  # Group similar length sequences for efficiency
remove_unused_columns: true
ignore_data_skip: false
deepspeed: null  # Use FSDP instead of DeepSpeed for H100

# Additional optimizations
dataloader_drop_last: true
eval_delay: 0
save_safetensors: true 