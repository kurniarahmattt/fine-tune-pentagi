#!/bin/bash

set -e

echo "ðŸš€ Starting Qwen3:8B Security Fine-tuning on 4x H100 GPUs (Docker Environment)"
echo "ðŸ“Š Hardware: 4x NVIDIA H100 80GB HBM3"
echo "ðŸŽ¯ Dataset: Security-focused penetration testing data"
echo "ðŸ”§ Framework: Axolotl with QLoRA"
echo "ðŸ’¾ Storage: /raid/workspace/rahmat/"

# Create necessary directories in raid
mkdir -p /raid/workspace/rahmat/logs
mkdir -p /raid/workspace/rahmat/data
mkdir -p /raid/workspace/rahmat/models
mkdir -p /raid/workspace/rahmat/.cache/huggingface
mkdir -p /raid/workspace/rahmat/axolotl_cache

# Create logs directory
mkdir -p /workspace/logs

# Function to log messages
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a /workspace/logs/training.log
}

log "ðŸ”§ Installing Axolotl and dependencies..."

# Install system dependencies
sudo apt-get update -qq
sudo apt-get install -y git ninja-build

# Install PyTorch with CUDA 12.2 support (optimized for H100)
log "ðŸ“¦ Installing PyTorch with CUDA 12.2..."
pip install --upgrade pip
MAX_JOBS=32 pip install --upgrade torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu122

# Install core dependencies
log "ðŸ“¦ Installing core dependencies..."
pip install --upgrade transformers==4.41.0 datasets==2.20.0 accelerate==0.30.1 peft==0.11.0 bitsandbytes==0.43.0
pip install --upgrade wandb huggingface_hub

# Install Axolotl from source (latest version)
log "ðŸ“¦ Installing Axolotl from source..."
rm -rf axolotl
git clone https://github.com/axolotl-ai-cloud/axolotl.git
cd axolotl
pip install -q -U packaging setuptools wheel ninja
MAX_JOBS=32 pip install --no-build-isolation -e .
python scripts/cutcrossentropy_install.py | sh

# Install Flash Attention for H100 optimization
log "ðŸ“¦ Installing Flash Attention for H100 optimization..."
MAX_JOBS=32 pip uninstall flash-attn -y
MAX_JOBS=32 pip install -q flash-attn==2.8.1 --no-build-isolation

# Install additional optimizations
log "ðŸ“¦ Installing additional optimizations..."
pip install --upgrade -q 'optree>=0.13.0'
pip install --upgrade pyarrow

cd ..

# Verify installations
log "âœ… Verifying installations..."
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import axolotl; print(f'Axolotl: {axolotl.__version__}')"

# Security enhancement and data preparation
log "ðŸ”’ Running security enhancement..."
python src/security_enhancement.py

log "ðŸ“Š Running data filtering..."
python src/data_filtering.py

log "ðŸ”„ Running data augmentation..."
python src/data_augmentation.py

# Display dataset statistics
log "ðŸ“ˆ Dataset Statistics:"
echo "Original: $(wc -l < ./data/training_data.jsonl) examples" | tee -a /workspace/logs/training.log
echo "Filtered: $(wc -l < ./data/filtered_data.jsonl) examples" | tee -a /workspace/logs/training.log
echo "Augmented: $(wc -l < ./data/augmented_training_data.jsonl) examples" | tee -a /workspace/logs/training.log

# Validate dataset format
log "ðŸ” Validating dataset format..."
python src/validate_dataset.py

# Set environment variables for optimal H100 performance
export CUDA_VISIBLE_DEVICES=0,1,2,3
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=0
export NCCL_P2P_DISABLE=0
export OMP_NUM_THREADS=8
export TOKENIZERS_PARALLELISM=false

# Update config to use raid directories
log "ðŸ”§ Updating configuration for raid storage..."
sed -i 's|output_dir: ./qwen3-finetuned|output_dir: /raid/workspace/rahmat/models/qwen3-finetuned|g' config/qwen3_h100_4gpu_config.yml
sed -i 's|logging_dir: ./logs|logging_dir: /raid/workspace/rahmat/logs|g' config/qwen3_h100_4gpu_config.yml

# Launch distributed training across 4 H100 GPUs
log "ðŸš€ Starting distributed training on 4x H100 GPUs..."
log "ðŸ“‹ Training Configuration:"
log "   - Model: Qwen3:8B with QLoRA"
log "   - GPUs: 4x H100 80GB"
log "   - Batch Size: 32 (micro_batch_size=4, gradient_accumulation=8)"
log "   - Sequence Length: 8192"
log "   - Learning Rate: 0.0002"
log "   - Epochs: 8"
log "   - Output: /raid/workspace/rahmat/models/qwen3-finetuned"
log "   - Logs: /raid/workspace/rahmat/logs"

# Start training with comprehensive logging
torchrun \
    --nproc_per_node=4 \
    --master_port=29500 \
    -m axolotl.cli.train \
    config/qwen3_h100_4gpu_config.yml \
    2>&1 | tee -a /workspace/logs/training.log

log "âœ… Training completed successfully!"
log "ðŸ“ Model saved to: /raid/workspace/rahmat/models/qwen3-finetuned"
log "ðŸ“Š Training logs saved to: /raid/workspace/rahmat/logs"

# Display final model information
if [ -d "/raid/workspace/rahmat/models/qwen3-finetuned" ]; then
    log "ðŸ“‹ Final Model Information:"
    ls -la /raid/workspace/rahmat/models/qwen3-finetuned/
    echo "Model size: $(du -sh /raid/workspace/rahmat/models/qwen3-finetuned/ | cut -f1)" | tee -a /workspace/logs/training.log
fi

# Copy logs to raid directory
log "ðŸ“‹ Copying logs to raid directory..."
cp -r /workspace/logs/* /raid/workspace/rahmat/logs/ 2>/dev/null || true

log "ðŸŽ‰ Fine-tuning pipeline completed successfully!"
log "ðŸ’¾ All data, logs, and models saved to /raid/workspace/rahmat/"